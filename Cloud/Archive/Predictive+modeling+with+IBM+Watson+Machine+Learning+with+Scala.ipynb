{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Predict outdoor equipment purchase with IBM Watson Machine Learning</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/blob/master/spark/product-line-prediction/images/products_graphics.png?raw=true\" alt=\"Icon\" width=\"800\"> </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to get data from the IBM Data Science Experience Community, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Scala is helpful. This notebook uses Scala 2.11 and Apache® Spark 2.0.\n",
    "\n",
    "You will use a publicly available data set, **GoSales Transactions for Naive Bayes Model**, which details anonymous outdoor equipment purchases. Use the details of this data set to predict clients' interests in terms of product line, such as golf accessories, camping equipment, and others.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Load a CSV file into an Apache® Spark DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache® Spark machine learning model.\n",
    "-  Train and evaluate a model.\n",
    "-  Persist a model in a Watson Machine Learning repository.\n",
    "-  Deploy a model for online scoring using Wastson Machine Learning API.\n",
    "-  Score sample scoring data using the Watson Machine Learning API.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Setup](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create spark ml model](#model)\n",
    "4.\t[Persist model](#persistence)\n",
    "5.\t[Predict locally and visualize](#visualization)\n",
    "6.\t[Deploy and score in a Cloud](#scoring)\n",
    "7.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Setup\n",
    "\n",
    "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "1.  Create a [Watson Machine Learning Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered). \n",
    "1.  Upload **GoSales Transactions** data as a data asset in IBM Data Science Experience.\n",
    "1.  Make sure that you are using a Spark 2.0 kernel.\n",
    "1.  Insert a project token. Click the *More* icon and then click *Insert project token*. A project token cell appears at the beginning of this notebook.\n",
    "1.  Run the project token cell by placing your cursor in the cell and then clicking *Run Cell, Select Below*.\n",
    "\n",
    "\n",
    "### Create the GoSales Transactions Data Asset  \n",
    "\n",
    "The GOSales data is a freely available data set on the Data Science Experience home page.\n",
    "\n",
    "1.  Go to the [GoSales Transactions for Naive Bayes Model](https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600) data card on the Data Science Experience **Community** page and open the card by double-clicking it.\n",
    "2.  Click the link icon.\n",
    "4.  Select the link, copy it by pressing Ctrl+C, and then, click **Close**.\n",
    "5.  In the following cell, replace the **link_to_data** variable value with the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and explore data\n",
    "\n",
    "In this section you will load the data as an Apache® Spark DataFrame and perform a basic exploration.\n",
    "\n",
    "Load the data to the Spark DataFrame by using *wget* to upload the data to gpfs and then *read* method. \n",
    "\n",
    "**Example**: First, you need to install required packages. You can do it by running the following code. Run it only one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scala.sys.process._\n",
    "\n",
    "\"wget https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600\".!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val filename = \"data?accessKey=9561295fa407698694b1e254d0099600\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val df_data = spark.\n",
    "    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    load(filename)\n",
    "df_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the loaded data by using the following Apache® Spark DataFrame methods:\n",
    "-  print schema\n",
    "-  print top ten records\n",
    "-  count all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data contains five fields. PRODUCT_LINE field is the one we would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data set contains 60252 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create an Apache® Spark machine learning model\n",
    "\n",
    "In this section you will learn how to prepare data, create and train an Apache® Spark machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Prepare data\n",
    "\n",
    "In this subsection you will split your data into: train, test and predict data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val splits = df_data.randomSplit(Array(0.8, 0.18, 0.02), seed = 24L)\n",
    "val training_data = splits(0).cache()\n",
    "val test_data = splits(1)\n",
    "val prediction_data = splits(2)\n",
    "\n",
    "println(\"Number of training records: \", training_data.count())\n",
    "println(\"Number of testing records : \", test_data.count())\n",
    "println(\"Number of prediction records : \", prediction_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our data has been successfully split into three data sets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Create pipeline and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will create an Apache® Spark machine learning pipeline and then train the model.\n",
    "\n",
    "In the first step you need to import the Apache® Spark machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Spark ML libraries\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, IndexToString, VectorAssembler}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.{Model, Pipeline, PipelineStage, PipelineModel}\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, convert all the string fields to numeric ones by using the StringIndexer transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val stringIndexer_label = new StringIndexer().setInputCol(\"PRODUCT_LINE\").setOutputCol(\"label\").fit(df_data)\n",
    "val stringIndexer_prof = new StringIndexer().setInputCol(\"PROFESSION\").setOutputCol(\"PROFESSION_IX\")\n",
    "val stringIndexer_gend = new StringIndexer().setInputCol(\"GENDER\").setOutputCol(\"GENDER_IX\")\n",
    "val stringIndexer_mar = new StringIndexer().setInputCol(\"MARITAL_STATUS\").setOutputCol(\"MARITAL_STATUS_IX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, create a feature vector by combining all features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val vectorAssembler_features = new VectorAssembler().setInputCols(Array(\"GENDER_IX\", \"AGE\", \"MARITAL_STATUS_IX\", \"PROFESSION_IX\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define estimators you want to use for classification. Random Forest is used in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setNumTrees(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(stringIndexer_label.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val pipeline_rf = new Pipeline().setStages(Array(stringIndexer_label, stringIndexer_prof, stringIndexer_gend, stringIndexer_mar, vectorAssembler_features, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can train your Random Forest model by using the previously defined **pipeline** and **training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val model_rf = pipeline_rf.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your **model accuracy** now. To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val predictions = model_rf.transform(test_data)\n",
    "val evaluatorRF = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "val accuracy = evaluatorRF.evaluate(predictions)\n",
    "println(\"Accuracy = \" + accuracy)\n",
    "println(\"Test Error = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tune your model now to achieve better accuracy. For simplicity of this example tuning section is omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"persistence\"></a>\n",
    "## 4. Persist model\n",
    "\n",
    "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using Scala client libraries.\n",
    "\n",
    "First, you must import client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// WML client library\n",
    "\n",
    "import com.ibm.analytics.ngp.repository._\n",
    "\n",
    "// Helper libraries\n",
    "\n",
    "import scalaj.http.{Http, HttpOptions}\n",
    "import scala.util.{Success, Failure}\n",
    "import java.util.Base64\n",
    "import java.nio.charset.StandardCharsets\n",
    "import play.api.libs.json._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to Watson Machine Learning service on Bluemix.\n",
    "\n",
    "**Action**: Put authentication information from your instance of Watson Machine Learning service here.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val service_path = \"https://ibm-watson-ml.mybluemix.net\"\n",
    "val user = \"copy_your_username_here\"\n",
    "val password = \"copy_your_password_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: service_path, user and password can be found on **Service Credentials** tab of service instance created in Bluemix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val client = MLRepositoryClient(service_path)\n",
    "client.authorize(user, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model artifact (abstraction layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val model_artifact = MLRepositoryArtifact(model_rf, training_data, \"WML Product Line Prediction Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1: Save pipeline and model\n",
    "\n",
    "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val saved_model = client.models.save(model_artifact).get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get saved model metadata from Watson Machine Learning.\n",
    "\n",
    "**Tip**: Use *meta.availableProps* to get the list of available props."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_model.meta.availableProps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"modelType: \" + saved_model.meta.prop(\"modelType\"))\n",
    "println(\"trainingDataSchema: \" + saved_model.meta.prop(\"trainingDataSchema\"))\n",
    "println(\"creationTime: \" + saved_model.meta.prop(\"creationTime\"))\n",
    "println(\"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\"))\n",
    "println(\"label: \" + saved_model.meta.prop(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: **modelVersionHref** is our model unique indentifier in the Watson Machine Learning repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Load model and make predictions\n",
    "\n",
    "In this subsection you will learn how to load back saved model from specified instance of Watson Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val model_version_href = saved_model.meta.prop(\"modelVersionHref\").get\n",
    "val loaded_model_artifact = client.models.version(model_version_href).get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print for example model name to make sure that model artifact has been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model_artifact.name.mkString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the name is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaded_model_artifact match {\n",
    "        case SparkPipelineModelLoader(Success(model)) => {\n",
    "          val predictions = model.transform(prediction_data)\n",
    "        }\n",
    "        case SparkPipelineModelLoader(Failure(e)) => \"Loading failed.\"\n",
    "        case _ => println(s\"Unexpected artifact class: ${loaded_model_artifact.getClass}\")\n",
    "    }\n",
    "predictions.select(\"GENDER\", \"AGE\", \"MARITAL_STATUS\", \"PROFESSION\",\"predictedLabel\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tabulating a count, you can see which product line is the most popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already learned how save and load the model from Watson Machine Learning repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scoring\"></a>\n",
    "## 5. Deploy and score in a Cloud\n",
    "\n",
    "In this section you will learn how to create online scoring and to score a new data record by using the Watson Machine Learning REST API. \n",
    "For more information about REST APIs, see the [Swagger Documentation](http://watson-ml-api.mybluemix.net/).\n",
    "\n",
    "To work with the Watson Machine Leraning REST API you must generate an access token. To do that you can use the following sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Get WML service instance token\n",
    "\n",
    "val wml_auth_header = \"Basic \" + Base64.getEncoder.encodeToString((user + \":\" + password).getBytes(StandardCharsets.UTF_8))\n",
    "val wml_url = service_path + \"/v2/identity/token\"\n",
    "val wml_response = Http(wml_url).header(\"Authorization\", wml_auth_header).asString\n",
    "val wmltoken_json: JsValue = Json.parse(wml_response.body)\n",
    "\n",
    "val wmltoken = (wmltoken_json \\ \"token\").asOpt[String] match {\n",
    "    case Some(x) => x\n",
    "    case None => \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wmltoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Create online scoring endpoint\n",
    "\n",
    "Now you can create an online scoring endpoint. Execute the following sample code that uses the **modelVersionHref** value to create the scoring endpoint to the Bluemix repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val endpoint_online = \"https://ibm-watson-ml.stage1.mybluemix.net/v2/online/deployments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val payload_artifactVersionHref = loaded_model_artifact.meta.prop(\"modelVersionHref\").get\n",
    "val payload_name = \"Online scoring\"\n",
    "val payload_data_online = Json.stringify(Json.toJson(Map(\"artifactVersionHref\" -> payload_artifactVersionHref, \"name\" -> payload_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val response_online = Http(endpoint_online).postData(payload_data_online).header(\"Content-Type\", \"application/json\").header(\"Authorization\", wmltoken).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val scoring_href_json: JsValue = Json.parse(response_online.body)\n",
    "val scoring_href = (scoring_href_json \\ \"entity\" \\ \"scoringHref\").asOpt[String] match {\n",
    "    case Some(x) => x\n",
    "    case None => \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(scoring_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val payload_scoring = Json.stringify(Json.toJson(Map(\"record\" -> Json.toJson(List(Json.toJson(\"M\"), Json.toJson(55), Json.toJson(\"Single\"), Json.toJson(\"Executive\"))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payload_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can send (PUT) new scoring records (new data) for which you would like to get predictions. To do that, execute the following sample code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val response_scoring = Http(scoring_href).postData(payload_scoring).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wmltoken).option(HttpOptions.method(\"PUT\")).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(response_scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we predict that a 55-year-old single male executive is interested in Camping Equipment (predictedLabel: Camping Equipment, prediction: 0.0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps \n",
    "\n",
    "You successfully completed this notebook! You learned how to use Apache Spark machine learning as well as Watson Machine Learning for model creation and deployment. Check out our [Online Documentation](http://datascience.ibm.com/docs/content/getting-started/get-started.html) for more samples, tutorials, documentation, how-tos, and blog posts. \n",
    " \n",
    "### Authors\n",
    "\n",
    "**Umit Mert Cakmak** is Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable insights.\n",
    "\n",
    "Copyright © 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.0",
   "language": "scala",
   "name": "scala-spark20"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}